{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y tensorflow tensorflow-gpu tf-keras keras flash-attn\n",
    "%pip install -q torch torchvision\n",
    "%pip install -q transformers accelerate\n",
    "%pip install -q qwen-vl-utils pillow requests\n",
    "%pip install -q pdf2image pymupdf pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n## Step 1: Import Dependencies for Qwen2.5 Vision-Language Model\n\nThis cell imports all required libraries for running the Qwen2.5 model, which is a\nvision-language model that processes both text and images. We use AutoModelForVision2Seq\ninstead of the standard AutoModel because Qwen2.5 is specifically designed for\nvision-to-sequence tasks (e.g., image captioning, visual question answering).\n\"\"\"\nimport time\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom PIL import Image\nimport fitz  # PyMuPDF - Used for PDF processing and image extraction\nimport os\nimport json\nimport traceback\nfrom datetime import datetime\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch.nn.functional as F\n\n# Verify library versions and hardware availability\n# This helps ensure compatibility and diagnose potential issues before model loading\nprint(\"Libraries loaded successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Transformers version: {__import__('transformers').__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Display GPU information if available\n# GPU acceleration is critical for efficient inference with large vision-language models\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 2: Load Qwen2.5 Vision-Language Model\n",
    "\n",
    "This cell loads the model and processor. Only run this cell once per session\n",
    "to avoid redundant loading and memory allocation.\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "print(\"Loading model from checkpoint...\")\n",
    "\n",
    "# Load the vision-language model with optimized settings\n",
    "# - dtype=torch.bfloat16: Uses BFloat16 precision to reduce memory usage by ~50%\n",
    "#   while maintaining numerical stability better than FP16. Critical for fitting\n",
    "#   large models on consumer GPUs.\n",
    "# - device_map=\"auto\": Automatically distributes model layers across available\n",
    "#   GPU(s) and CPU memory, enabling efficient use of hardware resources.\n",
    "# - trust_remote_code=True: Allows execution of custom modeling code from the\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the processor which handles tokenization and image preprocessing\n",
    "# The processor ensures inputs are formatted correctly for the model's expected input structure\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Display VRAM usage to monitor memory consumption\n",
    "# This helps identify potential out-of-memory issues and track resource utilization\n",
    "print(f\"Current VRAM allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 3: PDF to Images Conversion using PyMuPDF\n",
    "\n",
    "These functions handle PDF processing and image preprocessing for optimal OCR performance.\n",
    "PyMuPDF (fitz) is used because it provides faster rendering and better memory efficiency\n",
    "compared to alternatives like pdf2image, and doesn't require external dependencies like Poppler.\n",
    "\"\"\"\n",
    "\n",
    "def pdf_to_images(pdf_path: str, dpi: int = 300) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert each page of a PDF document into a PIL Image.\n",
    "\n",
    "    This function uses PyMuPDF's rendering engine to convert PDF pages to raster images.\n",
    "    Higher DPI values produce better quality but increase memory usage and processing time.\n",
    "    300 DPI is chosen as default because it provides a good balance between quality and\n",
    "    performance for most OCR tasks.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Absolute or relative path to the PDF file\n",
    "        dpi: Dots per inch for rendering. Standard values are:\n",
    "             - 72: Screen quality (fast, lower quality)\n",
    "             - 150: Acceptable for basic OCR\n",
    "             - 300: High quality for accurate OCR (recommended)\n",
    "             - 600: Very high quality for small text\n",
    "\n",
    "    Returns:\n",
    "        List of PIL Images in RGB format, one image per page\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the PDF file doesn't exist\n",
    "        fitz.FileDataError: If the file is not a valid PDF\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "    print(f\"Converting PDF to images at {dpi} DPI...\")\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "\n",
    "    # Calculate zoom factor from desired DPI\n",
    "    # PyMuPDF uses 72 DPI as base resolution, so we scale relative to that\n",
    "    zoom = dpi / 72.0\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "    try:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "\n",
    "            # Render page to pixmap (raster image)\n",
    "            # alpha=False removes transparency channel to save memory and ensure RGB output\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "\n",
    "            # Convert PyMuPDF pixmap to PIL Image\n",
    "            # This conversion is necessary because the model processor expects PIL Images\n",
    "            img = Image.frombytes(\n",
    "                \"RGB\",\n",
    "                [pix.width, pix.height],\n",
    "                pix.samples\n",
    "            )\n",
    "            images.append(img)\n",
    "\n",
    "            print(f\"  Processed page {page_num + 1}/{len(doc)}: {pix.width}x{pix.height}px\")\n",
    "\n",
    "    finally:\n",
    "        # Ensure document is closed even if an error occurs\n",
    "        # This prevents memory leaks from unclosed file handles\n",
    "        doc.close()\n",
    "\n",
    "    print(f\"Successfully converted {len(images)} pages\")\n",
    "    return images\n",
    "\n",
    "\n",
    "def preprocess_image_for_ocr(\n",
    "    image: Image.Image,\n",
    "    max_size: int = 2048\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize images that exceed maximum dimensions to prevent memory issues.\n",
    "\n",
    "    Large images can cause out-of-memory errors during model inference and don't\n",
    "    necessarily improve OCR accuracy. This function downscales oversized images while\n",
    "    maintaining aspect ratio. LANCZOS resampling is used because it provides the best\n",
    "    quality for downscaling, preserving text clarity better than other methods.\n",
    "\n",
    "    Args:\n",
    "        image: Input PIL Image in any mode\n",
    "        max_size: Maximum allowed dimension (width or height) in pixels.\n",
    "                  2048 is chosen as a reasonable upper bound that balances quality\n",
    "                  with memory constraints for most GPUs (typically uses ~4-6GB VRAM)\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed PIL Image, resized if necessary\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "\n",
    "    # Only resize if image exceeds maximum dimension\n",
    "    # This avoids unnecessary processing and potential quality loss for smaller images\n",
    "    if max(width, height) > max_size:\n",
    "        # Calculate scale factor to fit within max_size while preserving aspect ratio\n",
    "        scale = max_size / max(width, height)\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "\n",
    "        # LANCZOS provides highest quality downsampling, critical for preserving text legibility\n",
    "        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        print(f\"    Image resized: {width}x{height} -> {new_width}x{new_height}\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "print(\"PDF processing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 4: OCR Generation with Retry Logic and Confidence Scoring\n",
    "\n",
    "This cell implements a robust OCR pipeline with multiple inference attempts per page\n",
    "and confidence scoring to assess output quality. The selection strategy is optimized\n",
    "for medical pathology reports where accuracy is critical.\n",
    "\"\"\"\n",
    "\n",
    "# Generation hyperparameters optimized for OCR tasks\n",
    "TEMPERATURE_SCHEDULE = [0.1, 0.2, 0.3]\n",
    "TOP_P_THRESHOLD = 0.95\n",
    "REPETITION_PENALTY = 1.1\n",
    "\n",
    "# Medical OCR quality thresholds\n",
    "QUALITY_SCORE_THRESHOLD = 0.5  # Minimum acceptable composite quality score\n",
    "PERPLEXITY_THRESHOLD = 20.0     # Maximum acceptable perplexity\n",
    "MIN_PROB_THRESHOLD = 0.05       # Minimum acceptable token probability\n",
    "\n",
    "\n",
    "def calculate_confidence_scores(scores, generated_ids) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate various confidence metrics from model generation scores.\n",
    "\n",
    "    Confidence scores help assess the reliability of OCR output. Lower confidence\n",
    "    may indicate poor image quality, unusual fonts, or model uncertainty.\n",
    "\n",
    "    Args:\n",
    "        scores: Tuple of tensors containing logits for each generated token\n",
    "        generated_ids: The generated token IDs (1D tensor)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with confidence metrics:\n",
    "            - mean_probability: Average probability across all tokens (0-1)\n",
    "            - mean_log_probability: Average log probability (more numerically stable)\n",
    "            - perplexity: Model's uncertainty (lower is better)\n",
    "            - min_probability: Lowest token probability (identifies uncertain tokens)\n",
    "    \"\"\"\n",
    "    if not scores or len(scores) == 0:\n",
    "        return {\n",
    "            'mean_probability': None,\n",
    "            'mean_log_probability': None,\n",
    "            'perplexity': None,\n",
    "            'min_probability': None\n",
    "        }\n",
    "\n",
    "    # Convert logits to probabilities for each token position\n",
    "    # Logits are raw model outputs; softmax converts them to probability distributions\n",
    "    token_probs = []\n",
    "    for i, logits in enumerate(scores):\n",
    "        # Apply softmax to get probability distribution over vocabulary\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "\n",
    "        # Get probability of the actual generated token\n",
    "        token_id = generated_ids[i].item()\n",
    "        token_prob = probs[token_id].item()\n",
    "        token_probs.append(token_prob)\n",
    "\n",
    "    token_probs = np.array(token_probs)\n",
    "\n",
    "    # Calculate various confidence metrics\n",
    "    # Mean probability: Simple average, intuitive but can be skewed by very low values\n",
    "    mean_prob = float(np.mean(token_probs))\n",
    "\n",
    "    # Log probability: More stable for very small probabilities, commonly used in NLP\n",
    "    log_probs = np.log(token_probs + 1e-10)  # Add epsilon to avoid log(0)\n",
    "    mean_log_prob = float(np.mean(log_probs))\n",
    "\n",
    "    # Perplexity: Exponential of negative mean log probability\n",
    "    # Intuition: \"How surprised is the model?\" Lower perplexity = higher confidence\n",
    "    # Typical range: 1.0 (perfect) to 100+ (very uncertain)\n",
    "    perplexity = float(np.exp(-mean_log_prob))\n",
    "\n",
    "    # Minimum probability: Identifies the least confident token\n",
    "    # Useful for spotting specific problem areas in the output\n",
    "    min_prob = float(np.min(token_probs))\n",
    "\n",
    "    return {\n",
    "        'mean_probability': mean_prob,\n",
    "        'mean_log_probability': mean_log_prob,\n",
    "        'perplexity': perplexity,\n",
    "        'min_probability': min_prob\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_composite_score(response: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate composite quality score optimized for medical document OCR.\n",
    "    \n",
    "    For medical pathology reports, accuracy is paramount. This scoring function\n",
    "    heavily weights quality (80%) over completeness (20%) to avoid hallucinations.\n",
    "    These weights were chosen via trial-and-error.\n",
    "    \n",
    "    Args:\n",
    "        response: Dictionary containing OCR attempt results with confidence metrics\n",
    "        \n",
    "    Returns:\n",
    "        Float score from 0-1 where higher indicates better quality.\n",
    "        Returns 0 if confidence data is unavailable.\n",
    "    \"\"\"\n",
    "    chars = response['chars']\n",
    "    confidence = response['confidence']\n",
    "    temperature = response['temperature']\n",
    "    \n",
    "    # Require confidence data\n",
    "\n",
    "    if confidence.get('perplexity') is None:\n",
    "        return 0.0\n",
    "    \n",
    "    perplexity = confidence['perplexity']\n",
    "    min_prob = confidence.get('min_probability', 0)\n",
    "    \n",
    "    # Normalize character count (0-1 scale, assuming 2000 chars is a \"full page\")\n",
    "    normalized_length = min(chars / 2000.0, 1.0)\n",
    "    \n",
    "    # Convert perplexity to quality score (0-1 scale)\n",
    "    # Lower perplexity = higher quality\n",
    "    # Typical range: 1.0 (perfect) → 1.0, 10.0 → 0.52, 100 → 0.18\n",
    "    quality_score = 1.0 / (1.0 + np.log(max(perplexity, 1.0)))\n",
    "    \n",
    "    # Minimum token probability penalty\n",
    "    # Penalize outputs with very uncertain tokens (potential hallucinations)\n",
    "    min_prob_penalty = 1.0 if min_prob > MIN_PROB_THRESHOLD else 0.7\n",
    "    \n",
    "    # Temperature bias: prefer conservative (low temperature) outputs\n",
    "    # we favor certainty over exploration\n",
    "    temperature_penalty = 1.0 if temperature <= 0.1 else 0.95\n",
    "    \n",
    "    # Final composite score\n",
    "    # 80% quality (perplexity-based), 20% completeness (length-based)\n",
    "    # Applied penalties for uncertain tokens and higher temperatures\n",
    "    final_score = (\n",
    "        (0.80 * quality_score) + \n",
    "        (0.20 * normalized_length)\n",
    "    ) * min_prob_penalty * temperature_penalty\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "\n",
    "def select_best_ocr(all_responses: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Select best OCR attempt for documents with quality gating.\n",
    "    \n",
    "    This function implements a quality-first selection strategy optimized for\n",
    "    reports where accuracy is critical. It adds quality warning\n",
    "    flags when confidence falls below acceptable thresholds.\n",
    "    \n",
    "    Args:\n",
    "        all_responses: List of OCR attempt results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with best response and quality warning flags\n",
    "    \"\"\"\n",
    "    if not all_responses:\n",
    "        return None\n",
    "    \n",
    "    # Calculate scores for all responses\n",
    "    scored_responses = []\n",
    "    for response in all_responses:\n",
    "        score = calculate_composite_score(response)\n",
    "        scored_responses.append((score, response))\n",
    "    \n",
    "    # Select best by composite score\n",
    "    best_score, best_response = max(scored_responses, key=lambda x: x[0])\n",
    "    \n",
    "    # Add quality metadata\n",
    "    best_response['composite_score'] = best_score\n",
    "    \n",
    "    # Quality gate: flag if below thresholds\n",
    "    perplexity = best_response['confidence'].get('perplexity', float('inf'))\n",
    "    min_prob = best_response['confidence'].get('min_probability', 0)\n",
    "    \n",
    "    quality_warnings = []\n",
    "    \n",
    "    if best_score < QUALITY_SCORE_THRESHOLD:\n",
    "        quality_warnings.append(\n",
    "            f\"Low composite quality score: {best_score:.3f} (threshold: {QUALITY_SCORE_THRESHOLD})\"\n",
    "        )\n",
    "    \n",
    "    if perplexity > PERPLEXITY_THRESHOLD:\n",
    "        quality_warnings.append(\n",
    "            f\"High perplexity (uncertainty): {perplexity:.2f} (threshold: {PERPLEXITY_THRESHOLD})\"\n",
    "        )\n",
    "    \n",
    "    if min_prob < MIN_PROB_THRESHOLD:\n",
    "        quality_warnings.append(\n",
    "            f\"Very uncertain tokens detected: min_prob={min_prob:.4f} (threshold: {MIN_PROB_THRESHOLD})\"\n",
    "        )\n",
    "    \n",
    "    # Check consistency across attempts\n",
    "    if len(all_responses) > 1:\n",
    "        char_counts = [r['chars'] for r in all_responses]\n",
    "        cv = np.std(char_counts) / np.mean(char_counts) if np.mean(char_counts) > 0 else 0\n",
    "        if cv > 0.3:  # >30% variation\n",
    "            quality_warnings.append(\n",
    "                f\"High variance between attempts: {cv:.1%} - results inconsistent\"\n",
    "            )\n",
    "    \n",
    "    best_response['quality_warning'] = len(quality_warnings) > 0\n",
    "    best_response['quality_warnings'] = quality_warnings\n",
    "    \n",
    "    if quality_warnings:\n",
    "        best_response['warning_message'] = (\n",
    "            \"QUALITY ALERT - Manual review recommended:\\n\" + \n",
    "            \"\\n\".join(f\"  - {w}\" for w in quality_warnings)\n",
    "        )\n",
    "    \n",
    "    return best_response\n",
    "\n",
    "\n",
    "def ocr_with_retry(\n",
    "    image: Image.Image,\n",
    "    page_num: int = 1,\n",
    "    num_attempts: int = 3,\n",
    "    max_new_tokens: int = 2048,\n",
    "    use_cot: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform OCR on a single image with multiple attempts, temperature variations,\n",
    "    and confidence scoring optimized for medical documents.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image to perform OCR on\n",
    "        page_num: Page number for logging and result tracking\n",
    "        num_attempts: Number of OCR attempts with different temperatures\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        use_cot: Whether to use Chain-of-Thought prompting\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing page results with confidence scores and quality warnings\n",
    "    \"\"\"\n",
    "\n",
    "    if use_cot:\n",
    "        prompt = \"\"\"Extract all text from this medical pathology report image with perfect accuracy. This is a clinical document where precision is critical.\n",
    "\n",
    "Work systematically through these steps:\n",
    "\n",
    "1. Identify the document structure (patient info, diagnostic sections, lab values, tables)\n",
    "2. Read each section carefully from top to bottom, left to right\n",
    "3. Pay special attention to:\n",
    "   - Patient identifiers and demographics\n",
    "   - Diagnosis codes and staging information\n",
    "   - Numerical values (measurements, lab results, percentages)\n",
    "   - Dates and timestamps\n",
    "   - Physician names and signatures\n",
    "4. Preserve all formatting, line breaks, and special characters exactly\n",
    "5. Double-check all numerical values and medical terminology\n",
    "6. Output the complete text transcription exactly as shown\n",
    "\n",
    "Provide the full text transcription:\"\"\"\n",
    "    else:\n",
    "        prompt = \"Read and transcribe all text from this medical document image exactly as shown, preserving all formatting and structure.\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    all_responses = []\n",
    "\n",
    "    print(f\"\\nPage {page_num} - Running {num_attempts} OCR attempts...\")\n",
    "\n",
    "    for i in range(num_attempts):\n",
    "        temp = TEMPERATURE_SCHEDULE[i % len(TEMPERATURE_SCHEDULE)]\n",
    "        print(f\"  Attempt [{i+1}/{num_attempts}] (temperature={temp})...\", end=\" \")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                # Enable score output to get token probabilities\n",
    "                # return_dict_in_generate=True provides structured output with scores\n",
    "                # output_scores=True includes logits for each generated token\n",
    "                generation_output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temp,\n",
    "                    top_p=TOP_P_THRESHOLD,\n",
    "                    do_sample=temp > 0,\n",
    "                    repetition_penalty=REPETITION_PENALTY,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "\n",
    "            # Extract generated token IDs\n",
    "            generated_ids = generation_output.sequences\n",
    "\n",
    "            # Trim input tokens from output\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            # Decode tokens to text\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            char_count = len(output_text)\n",
    "            word_count = len(output_text.split())\n",
    "\n",
    "            # Calculate confidence scores from generation scores\n",
    "            confidence = calculate_confidence_scores(\n",
    "                generation_output.scores,\n",
    "                generated_ids_trimmed[0]\n",
    "            )\n",
    "\n",
    "            all_responses.append({\n",
    "                'attempt': i + 1,\n",
    "                'text': output_text,\n",
    "                'temperature': temp,\n",
    "                'time': elapsed,\n",
    "                'chars': char_count,\n",
    "                'words': word_count,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "            # Display confidence in output\n",
    "            perp_display = f\"perplexity={confidence['perplexity']:.2f}\" if confidence['perplexity'] else \"perplexity=N/A\"\n",
    "            print(f\"Success - {char_count} chars, {perp_display} ({elapsed:.1f}s)\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed - {str(e)}\")\n",
    "            # Print more detailed error for debugging\n",
    "            print(f\"  Error details: {traceback.format_exc()}\")\n",
    "            continue\n",
    "\n",
    "    if not all_responses:\n",
    "        return {\"error\": f\"All OCR attempts failed for page {page_num}\"}\n",
    "\n",
    "    # Select best response using  quality scoring\n",
    "    best = select_best_ocr(all_responses)\n",
    "\n",
    "    return {\n",
    "        'page_num': page_num,\n",
    "        'best_response': best,\n",
    "        'all_responses': all_responses,\n",
    "        'total_attempts': len(all_responses)\n",
    "    }\n",
    "\n",
    "\n",
    "def ocr_pdf(\n",
    "    pdf_path: str,\n",
    "    dpi: int = 300,\n",
    "    attempts_per_page: int = 3,\n",
    "    use_cot: bool = True,\n",
    "    max_pages: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform OCR on an entire PDF document with retry logic and confidence scoring per page.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        dpi: Resolution for PDF rendering\n",
    "        attempts_per_page: Number of OCR attempts per page\n",
    "        use_cot: Whether to use Chain-of-Thought prompting\n",
    "        max_pages: Optional limit on pages to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with results including per-page confidence scores and quality warnings\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PDF OCR WITH QUALITY GATING\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    images = pdf_to_images(pdf_path, dpi=dpi)\n",
    "\n",
    "    if max_pages:\n",
    "        images = images[:max_pages]\n",
    "        print(f\"Processing first {max_pages} pages only (limit applied)\")\n",
    "\n",
    "    total_pages = len(images)\n",
    "    print(f\"Total pages to process: {total_pages}\\n\")\n",
    "\n",
    "    all_results = []\n",
    "    pages_with_warnings = 0\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, image in enumerate(images, 1):\n",
    "        processed_img = preprocess_image_for_ocr(image)\n",
    "\n",
    "        result = ocr_with_retry(\n",
    "            image=processed_img,\n",
    "            page_num=i,\n",
    "            num_attempts=attempts_per_page,\n",
    "            max_new_tokens=2048,\n",
    "            use_cot=use_cot\n",
    "        )\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "        if 'best_response' in result:\n",
    "            best = result['best_response']\n",
    "            conf = best['confidence']\n",
    "            perp_str = f\"{conf['perplexity']:.2f}\" if conf['perplexity'] else \"N/A\"\n",
    "            score_str = f\"{best.get('composite_score', 0):.3f}\"\n",
    "            \n",
    "            print(f\"  Page {i}/{total_pages} completed: \"\n",
    "                  f\"{best['chars']} chars, perplexity={perp_str}, quality={score_str}\")\n",
    "            \n",
    "            # Display quality warnings if present\n",
    "            if best.get('quality_warning'):\n",
    "                pages_with_warnings += 1\n",
    "                print(f\"  {best['warning_message']}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  Page {i}/{total_pages} failed: {result.get('error', 'Unknown error')}\\n\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    full_text = \"\\n\\n\" + \"\\n\\n\".join([\n",
    "        f\"{'=' * 80}\\nPAGE {r['page_num']}\\n{'=' * 80}\\n{r['best_response']['text']}\"\n",
    "        for r in all_results if 'best_response' in r\n",
    "    ])\n",
    "\n",
    "    total_chars = sum(\n",
    "        r['best_response']['chars']\n",
    "        for r in all_results\n",
    "        if 'best_response' in r\n",
    "    )\n",
    "\n",
    "    # Calculate average perplexity across all pages\n",
    "    avg_perplexity = np.mean([\n",
    "        r['best_response']['confidence']['perplexity']\n",
    "        for r in all_results\n",
    "        if 'best_response' in r and r['best_response']['confidence']['perplexity'] is not None\n",
    "    ]) if all_results else None\n",
    "\n",
    "    # Calculate average quality score\n",
    "    avg_quality = np.mean([\n",
    "        r['best_response'].get('composite_score', 0)\n",
    "        for r in all_results\n",
    "        if 'best_response' in r\n",
    "    ]) if all_results else None\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PDF OCR COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Pages processed: {total_pages}\")\n",
    "    print(f\"Total characters: {total_chars:,}\")\n",
    "    print(f\"Average perplexity: {avg_perplexity:.2f}\" if avg_perplexity else \"Average perplexity: N/A\")\n",
    "    print(f\"Average quality score: {avg_quality:.3f}\" if avg_quality else \"Average quality score: N/A\")\n",
    "    print(f\"Pages with quality warnings: {pages_with_warnings}/{total_pages}\")\n",
    "    print(f\"Total time: {total_time:.1f}s (average: {total_time/total_pages:.1f}s per page)\")\n",
    "    \n",
    "    if pages_with_warnings > 0:\n",
    "        print(f\"\\nWARNING: {pages_with_warnings} page(s) flagged for manual review\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'pages': all_results,\n",
    "        'full_text': full_text,\n",
    "        'total_pages': total_pages,\n",
    "        'total_chars': total_chars,\n",
    "        'total_time': total_time,\n",
    "        'avg_perplexity': avg_perplexity,\n",
    "        'avg_quality_score': avg_quality,\n",
    "        'pages_with_warnings': pages_with_warnings\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"OCR functions with quality gating defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n## Step 5: Result Saving Functions\n\nThis cell implements functions to save OCR results to text and JSON formats.\nText files contain the extracted content, while JSON files preserve all metadata\nincluding confidence scores and quality warnings for detailed analysis.\n\"\"\"\n\n\ndef save_results(\n    results: Dict,\n    filename: str,\n    include_metadata: bool = True\n) -> None:\n    \"\"\"\n    Save OCR results to a text file with optional metadata.\n\n    Args:\n        results: Dictionary containing OCR results from ocr_pdf()\n        filename: Output file path (absolute or relative)\n        include_metadata: If True, prepend metadata header to output\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    try:\n        with open(filename, 'w', encoding='utf-8') as f:\n            if include_metadata:\n                # Write metadata header\n                f.write(\"=\" * 80 + \"\\n\")\n                f.write(\"OCR METADATA\\n\")\n                f.write(\"=\" * 80 + \"\\n\")\n                f.write(f\"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n                f.write(f\"Total Pages: {results.get('total_pages', 0)}\\n\")\n                f.write(f\"Total Characters: {results.get('total_chars', 0):,}\\n\")\n                \n                avg_perp = results.get('avg_perplexity')\n                if avg_perp:\n                    f.write(f\"Average Perplexity: {avg_perp:.2f}\\n\")\n                \n                avg_quality = results.get('avg_quality_score')\n                if avg_quality:\n                    f.write(f\"Average Quality Score: {avg_quality:.3f}\\n\")\n                \n                warnings = results.get('pages_with_warnings', 0)\n                if warnings > 0:\n                    f.write(f\"\\nQuality Warnings: {warnings} page(s) flagged for manual review\\n\")\n                \n                f.write(f\"Processing Time: {results.get('total_time', 0):.1f}s\\n\")\n                f.write(\"=\" * 80 + \"\\n\\n\")\n            \n            # Write full text content\n            f.write(results.get('full_text', ''))\n            \n        print(f\"Results saved to: {filename}\")\n        \n    except IOError as e:\n        print(f\"Error saving results to {filename}: {str(e)}\")\n        raise\n\n\ndef save_detailed_results(\n    results: Dict,\n    filename: str\n) -> None:\n    \"\"\"\n    Save comprehensive OCR results to JSON format including all attempts and metrics.\n\n    This function preserves all OCR attempts, confidence scores, quality warnings,\n    and metadata for detailed analysis and quality assurance workflows.\n\n    Args:\n        results: Dictionary containing OCR results from ocr_pdf()\n        filename: Output JSON file path (absolute or relative)\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    try:\n        # Prepare serializable output\n        output = {\n            'metadata': {\n                'processing_date': datetime.now().isoformat(),\n                'total_pages': results.get('total_pages', 0),\n                'total_chars': results.get('total_chars', 0),\n                'total_time_seconds': results.get('total_time', 0),\n                'avg_perplexity': results.get('avg_perplexity'),\n                'avg_quality_score': results.get('avg_quality_score'),\n                'pages_with_warnings': results.get('pages_with_warnings', 0)\n            },\n            'pages': []\n        }\n        \n        # Add per-page details\n        for page_result in results.get('pages', []):\n            if 'error' in page_result:\n                output['pages'].append({\n                    'page_num': page_result.get('page_num'),\n                    'error': page_result['error']\n                })\n            else:\n                page_data = {\n                    'page_num': page_result['page_num'],\n                    'total_attempts': page_result['total_attempts'],\n                    'best_response': {\n                        'attempt': page_result['best_response']['attempt'],\n                        'temperature': page_result['best_response']['temperature'],\n                        'chars': page_result['best_response']['chars'],\n                        'words': page_result['best_response']['words'],\n                        'time_seconds': page_result['best_response']['time'],\n                        'composite_score': page_result['best_response'].get('composite_score'),\n                        'confidence': page_result['best_response']['confidence'],\n                        'quality_warning': page_result['best_response'].get('quality_warning', False),\n                        'quality_warnings': page_result['best_response'].get('quality_warnings', []),\n                        'text': page_result['best_response']['text']\n                    },\n                    'all_attempts': [\n                        {\n                            'attempt': resp['attempt'],\n                            'temperature': resp['temperature'],\n                            'chars': resp['chars'],\n                            'words': resp['words'],\n                            'time_seconds': resp['time'],\n                            'confidence': resp['confidence']\n                        }\n                        for resp in page_result['all_responses']\n                    ]\n                }\n                output['pages'].append(page_data)\n        \n        # Write JSON with pretty formatting\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(output, f, indent=2, ensure_ascii=False)\n        \n        print(f\"Detailed results saved to: {filename}\")\n        \n    except IOError as e:\n        print(f\"Error saving detailed results to {filename}: {str(e)}\")\n        raise\n\n\nprint(\"Result saving functions defined successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n## Step 6: Batch Process PDFs in Directory Structure\n\nThis cell implements recursive directory traversal to process all PDFs found in\na folder hierarchy. Results are saved alongside source files to maintain organization.\n\"\"\"\n\n\ndef find_pdf_files(root_directory: str, skip_processed: bool = True) -> List[Tuple[str, str]]:\n    \"\"\"\n    Recursively find all PDF files in a directory and its subdirectories.\n\n    This function traverses the entire directory tree to locate PDFs while optionally\n    skipping files that have already been processed (i.e., have corresponding .txt files).\n    This prevents redundant processing in subsequent runs.\n\n    Args:\n        root_directory: Path to the root directory to search\n        skip_processed: If True, skip PDFs that already have corresponding .txt output files.\n                       This is useful for resuming interrupted batch jobs.\n\n    Returns:\n        List of tuples: (pdf_path, output_txt_path) for each PDF to process\n    \"\"\"\n    pdf_files = []\n\n    # Validate root directory exists\n    if not os.path.exists(root_directory):\n        raise FileNotFoundError(f\"Directory not found: {root_directory}\")\n\n    print(f\"Scanning directory: {root_directory}\")\n\n    # os.walk recursively yields (dirpath, dirnames, filenames) for each directory\n    # This is more efficient than recursive function calls for deep hierarchies\n    for dirpath, dirnames, filenames in os.walk(root_directory):\n        for filename in filenames:\n            # Case-insensitive PDF detection to handle .PDF, .pdf, .Pdf, etc.\n            if filename.lower().endswith('.pdf'):\n                pdf_path = os.path.join(dirpath, filename)\n\n                # Generate output filename by replacing .pdf extension with .txt\n                # This keeps the output in the same directory as the source\n                output_filename = os.path.splitext(filename)[0] + '_ocr.txt'\n                output_path = os.path.join(dirpath, output_filename)\n\n                # Skip if already processed (unless user wants to reprocess)\n                if skip_processed and os.path.exists(output_path):\n                    print(f\"  Skipping (already processed): {pdf_path}\")\n                    continue\n\n                pdf_files.append((pdf_path, output_path))\n\n    print(f\"Found {len(pdf_files)} PDF(s) to process\")\n    return pdf_files\n\n\ndef process_pdf_batch(\n    root_directory: str,\n    dpi: int = 300,\n    attempts_per_page: int = 3,\n    use_cot: bool = True,\n    max_pages: Optional[int] = None,\n    skip_processed: bool = True,\n    save_detailed: bool = False\n) -> Dict:\n    \"\"\"\n    Process all PDFs found in a directory tree with OCR.\n\n    This function orchestrates batch OCR processing across multiple files. It handles\n    errors gracefully so that one failed PDF doesn't stop the entire batch. Progress\n    is tracked and reported to help monitor long-running jobs.\n\n    Args:\n        root_directory: Root directory to search for PDFs\n        dpi: Resolution for PDF rendering\n        attempts_per_page: Number of OCR attempts per page\n        use_cot: Whether to use Chain-of-Thought prompting\n        max_pages: Optional limit on pages per PDF (useful for testing)\n        skip_processed: Skip PDFs that already have output files\n        save_detailed: If True, also save detailed JSON output with all attempts\n\n    Returns:\n        Dictionary with batch processing statistics\n    \"\"\"\n\n    print(\"=\" * 80)\n    print(\"BATCH PDF OCR PROCESSING\")\n    print(\"=\" * 80)\n\n    # Find all PDFs to process\n    pdf_files = find_pdf_files(root_directory, skip_processed=skip_processed)\n\n    if not pdf_files:\n        print(\"\\nNo PDFs found to process.\")\n        return {\n            'total_files': 0,\n            'successful': 0,\n            'failed': 0,\n            'skipped': 0\n        }\n\n    # Track batch statistics\n    total_files = len(pdf_files)\n    successful = 0\n    failed = 0\n    failed_files = []\n    batch_start = time.time()\n\n    print(f\"\\nProcessing {total_files} PDF file(s)...\\n\")\n\n    # Process each PDF individually\n    # Using enumerate for progress tracking\n    for idx, (pdf_path, output_path) in enumerate(pdf_files, 1):\n        print(\"=\" * 80)\n        print(f\"FILE {idx}/{total_files}: {os.path.basename(pdf_path)}\")\n        print(f\"Location: {os.path.dirname(pdf_path)}\")\n        print(\"=\" * 80)\n\n        try:\n            # Run OCR on the PDF\n            # Each file is processed independently to isolate errors\n            results = ocr_pdf(\n                pdf_path=pdf_path,\n                dpi=dpi,\n                attempts_per_page=attempts_per_page,\n                use_cot=use_cot,\n                max_pages=max_pages\n            )\n\n            # Save text output to the same directory as source PDF\n            # This maintains the organizational structure of the input\n            save_results(\n                results=results,\n                filename=output_path,\n                include_metadata=True\n            )\n\n            # Optionally save detailed JSON output with all metadata\n            # Useful for quality analysis or when you need access to all attempts\n            if save_detailed:\n                detailed_path = output_path.replace('_ocr.txt', '_ocr_detailed.json')\n                save_detailed_results(results, filename=detailed_path)\n\n            successful += 1\n            print(f\"\\nFile {idx}/{total_files} completed successfully\")\n\n        except Exception as e:\n            # Log error but continue processing remaining files\n            # This ensures one problematic PDF doesn't halt the entire batch\n            failed += 1\n            failed_files.append({\n                'file': pdf_path,\n                'error': str(e)\n            })\n\n            print(f\"\\nError processing {pdf_path}:\")\n            print(f\"  {str(e)}\")\n            print(\"\\nFull traceback:\")\n            print(traceback.format_exc())\n            print(\"\\nContinuing with next file...\")\n\n        # Clear GPU memory between files to prevent accumulation\n        # This is critical for processing large batches without memory errors\n        torch.cuda.empty_cache()\n        print()\n\n    batch_time = time.time() - batch_start\n\n    # Print final batch summary\n    print(\"\\n\" + \"=\" * 80)\n    print(\"BATCH PROCESSING COMPLETE\")\n    print(\"=\" * 80)\n    print(f\"Total files: {total_files}\")\n    print(f\"Successful: {successful}\")\n    print(f\"Failed: {failed}\")\n    print(f\"Total time: {batch_time:.1f}s ({batch_time/60:.1f}m)\")\n    \n    if failed_files:\n        print(f\"\\nFailed files:\")\n        for ff in failed_files:\n            print(f\"  - {ff['file']}: {ff['error']}\")\n    \n    print(\"=\" * 80)\n\n    return {\n        'total_files': total_files,\n        'successful': successful,\n        'failed': failed,\n        'failed_files': failed_files,\n        'batch_time_seconds': batch_time\n    }\n\n\nprint(\"Batch processing functions defined successfully!\")"
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\n## Step 7: Execute Batch Processing\n\nRun this cell to process all PDFs in the specified directory.\nAdjust the parameters below based on your needs.\n\"\"\"\n\n# Configure batch processing parameters\nresults = process_pdf_batch(\n    root_directory='reports',          # Directory containing PDF files\n    dpi=300,                            # Image resolution (300 recommended)\n    attempts_per_page=3,                # Number of retry attempts per page\n    skip_processed=True,                # Skip files that already have output\n    use_cot=True,                       # Use medical-optimized CoT prompting\n    max_pages=None,                     # Limit pages per PDF (None = all pages)\n    save_detailed=False                 # Save detailed JSON with all attempts\n)\n\n# Display batch statistics\nprint(f\"\\n\\nBatch Processing Summary:\")\nprint(f\"  Total files processed: {results['successful']}/{results['total_files']}\")\nprint(f\"  Processing time: {results['batch_time_seconds']/60:.1f} minutes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
